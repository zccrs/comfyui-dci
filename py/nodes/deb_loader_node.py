import os
import fnmatch
import tempfile
import tarfile
import io
from PIL import Image
import torch
import numpy as np
from ..utils.file_utils import load_binary_data
from ..utils.i18n import t
from .base_node import BaseNode

class DebLoader(BaseNode):
    """ComfyUI node for loading files from Debian packages with file filtering"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                t("deb_file_path"): ("STRING", {"default": "", "multiline": False}),
                t("file_filter"): ("STRING", {"default": "*.dci", "multiline": False}),
                t("skip_symlinks"): ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("BINARY_DATA_LIST", "STRING_LIST", "IMAGE", "STRING_LIST", "STRING_LIST")
    RETURN_NAMES = (t("binary_data_list"), t("relative_paths"), t("image_list"), t("image_relative_paths"), t("skipped_files"))
    FUNCTION = "execute"
    CATEGORY = f"DCI/{t('Files')}"

    def _execute(self, **kwargs):
        """Load files from deb package with filtering"""
        # Extract parameters with translation support
        # Try both translated and original parameter names for compatibility
        deb_file_path = kwargs.get(t("deb_file_path")) if t("deb_file_path") in kwargs else kwargs.get("deb_file_path", "")
        file_filter = kwargs.get(t("file_filter")) if t("file_filter") in kwargs else kwargs.get("file_filter", "*.dci")
        skip_symlinks = kwargs.get(t("skip_symlinks")) if t("skip_symlinks") in kwargs else kwargs.get("skip_symlinks", True)

        return self._execute_impl(deb_file_path, file_filter, skip_symlinks)

    def _execute_impl(self, deb_file_path="", file_filter="*.dci", skip_symlinks=True):
        """Load files from deb package with filtering"""

        # Validate deb file path
        if not deb_file_path:
            print("é”™è¯¯ï¼šæœªæä¾›debæ–‡ä»¶è·¯å¾„")
            return ([], [], [], [], [])

        # Normalize cross-platform path
        normalized_path = self._normalize_cross_platform_path(deb_file_path)

        # Check if file exists
        if not os.path.exists(normalized_path):
            print(f"é”™è¯¯ï¼šdebæ–‡ä»¶ä¸å­˜åœ¨: {normalized_path}")
            return ([], [], [], [], [])

        # Log file information
        file_size = os.path.getsize(normalized_path)
        print(f"æ­£åœ¨è§£ædebæ–‡ä»¶: {normalized_path}")
        print(f"æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size / 1024:.1f} KB)")
        print(f"æ–‡ä»¶è¿‡æ»¤å™¨: {file_filter}")
        if skip_symlinks:
            print("è·³è¿‡è½¯é“¾æ¥: å¯ç”¨")
        else:
            print("è·³è¿‡è½¯é“¾æ¥: ç¦ç”¨")

        # Parse deb file and extract all files
        import time
        start_time = time.time()
        try:
            all_files, skipped_symlinks = self._parse_deb_file(normalized_path, skip_symlinks)
            parse_time = time.time() - start_time
            print(f"ä»debåŒ…ä¸­æå– {len(all_files)} ä¸ªæ–‡ä»¶ (è€—æ—¶: {parse_time:.2f}ç§’)")
            if skipped_symlinks:
                print(f"è·³è¿‡ {len(skipped_symlinks)} ä¸ªè½¯é“¾æ¥æ–‡ä»¶")
                for symlink in skipped_symlinks:
                    print(f"  â­ï¸ {symlink}")
        except Exception as e:
            print(f"é”™è¯¯ï¼šè§£ædebæ–‡ä»¶å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return ([], [], [], [], [])

        # Filter files based on pattern
        filter_start_time = time.time()
        try:
            matching_files = self._filter_files(all_files, file_filter)
            filter_time = time.time() - filter_start_time
            print(f"è¿‡æ»¤ååŒ¹é… {len(matching_files)} ä¸ªæ–‡ä»¶ (è€—æ—¶: {filter_time:.3f}ç§’)")

            if len(matching_files) == 0:
                print("è­¦å‘Šï¼šæ²¡æœ‰æ–‡ä»¶åŒ¹é…è¿‡æ»¤æ¡ä»¶")
                print("å¯ç”¨çš„æ–‡ä»¶åˆ—è¡¨:")
                for file_path in sorted(all_files.keys())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"  {file_path}")
                if len(all_files) > 10:
                    print(f"  ... è¿˜æœ‰ {len(all_files) - 10} ä¸ªæ–‡ä»¶")
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ–‡ä»¶è¿‡æ»¤å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return ([], [], [], [], [])

        # Process results
        binary_data_list = []
        relative_paths = []
        image_list = []
        image_relative_paths = []
        successful_loads = 0
        successful_images = 0
        total_bytes = 0

        process_start_time = time.time()
        for file_path, file_content in matching_files.items():
            try:
                if file_content is not None:
                    binary_data_list.append(file_content)
                    relative_paths.append(file_path)
                    successful_loads += 1
                    total_bytes += len(file_content)
                    print(f"  âœ“ åŠ è½½æˆåŠŸ: {file_path} ({len(file_content):,} å­—èŠ‚)")

                    # Try to decode as image
                    image_tensor = self._try_decode_image(file_content, file_path)
                    if image_tensor is not None:
                        image_list.append(image_tensor)
                        image_relative_paths.append(file_path)
                        successful_images += 1
                        shape = image_tensor.shape
                        print(f"    âœ“ å›¾åƒè§£ç æˆåŠŸ: {file_path} (å°ºå¯¸: {shape[1]}x{shape[0]}x{shape[2]})")
                else:
                    print(f"  âŒ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path}")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¼‚å¸¸: {file_path} - {str(e)}")
                import traceback
                traceback.print_exc()

        process_time = time.time() - process_start_time
        total_time = time.time() - start_time

        # Summary statistics
        print(f"\n=== å¤„ç†ç»“æœæ‘˜è¦ ===")
        print(f"æˆåŠŸåŠ è½½: {successful_loads}/{len(matching_files)} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸè§£ç : {successful_images} ä¸ªå›¾åƒæ–‡ä»¶")
        print(f"æ€»æ•°æ®é‡: {total_bytes:,} å­—èŠ‚ ({total_bytes / 1024:.1f} KB)")
        print(f"å¤„ç†æ—¶é—´: è§£æ {parse_time:.2f}s + è¿‡æ»¤ {filter_time:.3f}s + å¤„ç† {process_time:.3f}s = æ€»è®¡ {total_time:.2f}s")

        if successful_loads > 0:
            avg_file_size = total_bytes / successful_loads
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {avg_file_size:.0f} å­—èŠ‚")

        # Convert image list to ComfyUI format or empty list
        if image_list:
            # Stack all images into a batch tensor
            images_tensor = torch.stack(image_list, dim=0)
            print(f"å›¾åƒæ‰¹æ¬¡å¼ é‡å½¢çŠ¶: {images_tensor.shape}")
            return (binary_data_list, relative_paths, images_tensor, image_relative_paths, skipped_symlinks)
        else:
            return (binary_data_list, relative_paths, [], [], skipped_symlinks)

    def _normalize_cross_platform_path(self, path):
        """Normalize path for cross-platform compatibility"""
        try:
            # Handle Windows paths on Linux/Unix systems
            if os.name != 'nt' and ':' in path and '\\' in path:
                # This looks like a Windows path on a Unix system
                print(f"æ£€æµ‹åˆ°Windowsè·¯å¾„æ ¼å¼: {path}")

                # Extract filename from Windows path
                filename = os.path.basename(path.replace('\\', '/'))
                print(f"æå–æ–‡ä»¶å: {filename}")

                # Search in common locations
                search_paths = [
                    '/tmp',
                    os.path.expanduser('~'),
                    os.getcwd(),
                    '/home',
                    '/var/tmp'
                ]

                for search_dir in search_paths:
                    if os.path.exists(search_dir):
                        candidate_path = os.path.join(search_dir, filename)
                        if os.path.exists(candidate_path):
                            print(f"æ‰¾åˆ°æ–‡ä»¶: {candidate_path}")
                            return candidate_path

                # If not found, try the original path converted to Unix format
                unix_path = '/' + path.replace('\\', '/').replace(':', '')
                if os.path.exists(unix_path):
                    print(f"ä½¿ç”¨è½¬æ¢åçš„Unixè·¯å¾„: {unix_path}")
                    return unix_path

                print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°å¯¹åº”æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„: {path}")
                return path
            else:
                # Normal path normalization
                return os.path.normpath(path.strip())

        except Exception as e:
            print(f"è·¯å¾„è§„èŒƒåŒ–å¤±è´¥: {str(e)}")
            return path

    def _parse_deb_file(self, deb_file_path, skip_symlinks=True):
        """Parse deb file and extract all files using pure Python implementation"""
        all_files = {}
        skipped_symlinks = []

        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                print(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_dir}")

                # Extract deb package using pure Python ar implementation
                extract_dir = os.path.join(temp_dir, "extract")
                os.makedirs(extract_dir)

                # Extract ar archive using pure Python
                print("å¼€å§‹è§£æarå½’æ¡£...")
                if not self._extract_ar_archive_python(deb_file_path, extract_dir):
                    print("é”™è¯¯ï¼šæ— æ³•è§£å‹debåŒ…")
                    return all_files, skipped_symlinks

                # List extracted files
                extracted_files = os.listdir(extract_dir)
                print(f"arå½’æ¡£è§£æå®Œæˆï¼Œæå–äº† {len(extracted_files)} ä¸ªæ–‡ä»¶:")
                for filename in extracted_files:
                    file_path = os.path.join(extract_dir, filename)
                    file_size = os.path.getsize(file_path)
                    print(f"  {filename}: {file_size:,} å­—èŠ‚")

                # Parse control.tar.*
                control_files = [f for f in extracted_files if f.startswith("control.tar")]
                if control_files:
                    control_tar = os.path.join(extract_dir, control_files[0])
                    print(f"è§£ææ§åˆ¶å½’æ¡£: {control_files[0]}")
                    control_file_dict, control_symlinks = self._extract_tar_files(control_tar, "control", skip_symlinks)
                    all_files.update(control_file_dict)
                    skipped_symlinks.extend(control_symlinks)
                    print(f"ä»control.tarä¸­æå– {len(control_file_dict)} ä¸ªæ–‡ä»¶")
                    if control_symlinks:
                        print(f"ä»control.tarä¸­è·³è¿‡ {len(control_symlinks)} ä¸ªè½¯é“¾æ¥")
                else:
                    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°control.taræ–‡ä»¶")

                # Parse data.tar.*
                data_files_list = [f for f in extracted_files if f.startswith("data.tar")]
                if data_files_list:
                    data_tar = os.path.join(extract_dir, data_files_list[0])
                    print(f"è§£ææ•°æ®å½’æ¡£: {data_files_list[0]}")
                    data_file_dict, data_symlinks = self._extract_tar_files(data_tar, "data", skip_symlinks)
                    all_files.update(data_file_dict)
                    skipped_symlinks.extend(data_symlinks)
                    print(f"ä»data.tarä¸­æå– {len(data_file_dict)} ä¸ªæ–‡ä»¶")
                    if data_symlinks:
                        print(f"ä»data.tarä¸­è·³è¿‡ {len(data_symlinks)} ä¸ªè½¯é“¾æ¥")
                else:
                    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°data.taræ–‡ä»¶")

                print(f"debåŒ…è§£æå®Œæˆï¼Œæ€»è®¡æå– {len(all_files)} ä¸ªæ–‡ä»¶")

        except Exception as e:
            print(f"é”™è¯¯ï¼šè§£ædebæ–‡ä»¶å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

        return all_files, skipped_symlinks

    def _extract_ar_archive_python(self, deb_file_path, extract_dir):
        """Extract ar archive using pure Python implementation"""
        try:
            print("ä½¿ç”¨çº¯Pythonå®ç°è§£æarå½’æ¡£...")
            file_count = 0
            total_extracted_size = 0

            with open(deb_file_path, 'rb') as f:
                # Read ar header
                magic = f.read(8)
                if magic != b'!<arch>\n':
                    print(f"é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆçš„arå½’æ¡£æ–‡ä»¶ï¼Œé­”æ•°: {magic}")
                    return False

                print("âœ“ arå½’æ¡£é­”æ•°éªŒè¯é€šè¿‡")

                while True:
                    # Read file header (60 bytes)
                    header_pos = f.tell()
                    header = f.read(60)
                    if len(header) < 60:
                        print(f"åˆ°è¾¾æ–‡ä»¶æœ«å°¾ï¼Œä½ç½®: {header_pos}")
                        break

                    # Parse header fields
                    filename_raw = header[0:16].decode('ascii')
                    date_field = header[16:28].decode('ascii').strip()
                    uid_field = header[28:34].decode('ascii').strip()
                    gid_field = header[34:40].decode('ascii').strip()
                    mode_field = header[40:48].decode('ascii').strip()
                    size_str = header[48:58].decode('ascii').strip()
                    end_marker = header[58:60]

                    # Clean filename
                    filename = filename_raw.strip().rstrip('/')

                    print(f"\n--- è§£ææ–‡ä»¶å¤´ #{file_count + 1} (ä½ç½®: {header_pos}) ---")
                    print(f"åŸå§‹æ–‡ä»¶å: '{filename_raw}' -> æ¸…ç†å: '{filename}'")
                    print(f"æ—¥æœŸ: {date_field}, UID: {uid_field}, GID: {gid_field}")
                    print(f"æƒé™: {mode_field}, ç»“æŸæ ‡è®°: {end_marker}")

                    if not size_str:
                        print("è­¦å‘Šï¼šæ–‡ä»¶å¤§å°å­—æ®µä¸ºç©ºï¼Œè·³è¿‡")
                        break

                    try:
                        size = int(size_str)
                        print(f"æ–‡ä»¶å¤§å°: {size:,} å­—èŠ‚")
                    except ValueError:
                        print(f"é”™è¯¯ï¼šæ— æ•ˆçš„æ–‡ä»¶å¤§å°: '{size_str}'")
                        break

                    # Read file content
                    content_pos = f.tell()
                    content = f.read(size)
                    if len(content) != size:
                        print(f"è­¦å‘Šï¼šè¯»å–çš„å†…å®¹å¤§å° ({len(content)}) ä¸é¢„æœŸä¸ç¬¦ ({size})")

                    # Pad to even boundary
                    if size % 2 == 1:
                        padding = f.read(1)
                        print(f"è¯»å–å¡«å……å­—èŠ‚: {padding}")

                    # Save file
                    if filename and not filename.startswith('/'):
                        output_path = os.path.join(extract_dir, filename)
                        try:
                            with open(output_path, 'wb') as out_f:
                                out_f.write(content)
                            file_count += 1
                            total_extracted_size += size
                            print(f"âœ“ æå–æ–‡ä»¶: {filename} ({size:,} å­—èŠ‚) -> {output_path}")
                        except Exception as e:
                            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {filename} - {str(e)}")
                    else:
                        print(f"â­ï¸ è·³è¿‡æ–‡ä»¶: '{filename}' (æ— æ•ˆæ–‡ä»¶å)")

            print(f"\n=== arå½’æ¡£è§£æå®Œæˆ ===")
            print(f"æå–æ–‡ä»¶æ•°é‡: {file_count}")
            print(f"æ€»æå–å¤§å°: {total_extracted_size:,} å­—èŠ‚ ({total_extracted_size / 1024:.1f} KB)")
            return True

        except Exception as e:
            print(f"Python arè§£æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def _extract_tar_files(self, tar_path, tar_type, skip_symlinks=True):
        """Extract all files from a tar archive"""
        files_dict = {}
        skipped_symlinks = []

        try:
            print(f"å¼€å§‹è§£æ {tar_type}.tar æ–‡ä»¶: {tar_path}")

            # Determine compression type and open accordingly
            compression_type = "æœªå‹ç¼©"
            if tar_path.endswith('.gz'):
                tar_file = tarfile.open(tar_path, 'r:gz')
                compression_type = "gzip"
            elif tar_path.endswith('.xz'):
                tar_file = tarfile.open(tar_path, 'r:xz')
                compression_type = "xz"
            elif tar_path.endswith('.bz2'):
                tar_file = tarfile.open(tar_path, 'r:bz2')
                compression_type = "bzip2"
            else:
                tar_file = tarfile.open(tar_path, 'r')

            print(f"å‹ç¼©æ ¼å¼: {compression_type}")

            with tar_file:
                members = tar_file.getmembers()
                print(f"tarå½’æ¡£åŒ…å« {len(members)} ä¸ªæ¡ç›®")

                file_count = 0
                dir_count = 0
                symlink_count = 0
                hardlink_count = 0
                total_size = 0

                for i, member in enumerate(members):
                    print(f"\n--- å¤„ç†æ¡ç›® #{i + 1}: {member.name} ---")
                    print(f"ç±»å‹: ", end="")

                    if member.isfile():
                        print(f"æ™®é€šæ–‡ä»¶ (å¤§å°: {member.size:,} å­—èŠ‚)")
                        file_count += 1
                        total_size += member.size
                    elif member.isdir():
                        print("ç›®å½•")
                        dir_count += 1
                    elif member.islnk():
                        print(f"ç¡¬é“¾æ¥ -> {member.linkname}")
                        hardlink_count += 1
                    elif member.issym():
                        print(f"è½¯é“¾æ¥ -> {member.linkname}")
                        symlink_count += 1
                    else:
                        print(f"å…¶ä»–ç±»å‹ (tarfileç±»å‹: {member.type})")

                    print(f"æƒé™: {oct(member.mode)}, UID: {member.uid}, GID: {member.gid}")
                    print(f"ä¿®æ”¹æ—¶é—´: {member.mtime}")

                    # Check if it's a symlink and should be skipped
                    if skip_symlinks and (member.islnk() or member.issym()):
                        clean_path = member.name.lstrip('./')
                        skipped_symlinks.append(clean_path)
                        if member.islnk():
                            print(f"  â­ï¸ è·³è¿‡ç¡¬é“¾æ¥: {clean_path} -> {member.linkname}")
                        else:
                            print(f"  â­ï¸ è·³è¿‡è½¯é“¾æ¥: {clean_path} -> {member.linkname}")
                        continue

                    if member.isfile():
                        try:
                            print(f"  ğŸ“„ å¼€å§‹æå–æ–‡ä»¶å†…å®¹...")
                            file_content = tar_file.extractfile(member).read()
                            # Use relative path without leading './'
                            clean_path = member.name.lstrip('./')
                            files_dict[clean_path] = file_content
                            print(f"  âœ“ æå–æˆåŠŸ: {clean_path} ({len(file_content):,} å­—èŠ‚)")
                        except Exception as e:
                            print(f"  âŒ æå–å¤±è´¥: {member.name} - {str(e)}")
                            files_dict[member.name.lstrip('./')] = None
                    else:
                        print(f"  â­ï¸ è·³è¿‡éæ–‡ä»¶æ¡ç›®")

                print(f"\n=== {tar_type}.tar è§£æå®Œæˆ ===")
                print(f"æ€»æ¡ç›®: {len(members)} (æ–‡ä»¶: {file_count}, ç›®å½•: {dir_count}, è½¯é“¾æ¥: {symlink_count}, ç¡¬é“¾æ¥: {hardlink_count})")
                print(f"æå–æ–‡ä»¶: {len(files_dict)} ä¸ª")
                print(f"è·³è¿‡é“¾æ¥: {len(skipped_symlinks)} ä¸ª")
                print(f"æ€»æ–‡ä»¶å¤§å°: {total_size:,} å­—èŠ‚ ({total_size / 1024:.1f} KB)")

        except Exception as e:
            print(f"é”™è¯¯ï¼šè§£æ{tar_type}.tarå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

        return files_dict, skipped_symlinks

    def _filter_files(self, all_files, file_filter):
        """Filter files based on the filter pattern"""
        matching_files = {}

        try:
            print(f"å¼€å§‹æ–‡ä»¶è¿‡æ»¤ï¼Œè¿‡æ»¤å™¨: '{file_filter}'")

            # Parse filter patterns
            patterns = [pattern.strip() for pattern in file_filter.replace(';', ',').split(',')]
            print(f"è§£æçš„è¿‡æ»¤æ¨¡å¼: {patterns}")

            matched_count = 0
            for file_path, file_content in all_files.items():
                # Get filename from path
                filename = os.path.basename(file_path)

                # Check if filename matches any pattern
                is_match = self._matches_filter(filename, patterns)

                if is_match:
                    matching_files[file_path] = file_content
                    matched_count += 1
                    print(f"  âœ“ åŒ¹é…: {file_path} (æ–‡ä»¶å: {filename})")
                else:
                    print(f"  âŒ ä¸åŒ¹é…: {file_path} (æ–‡ä»¶å: {filename})")

            print(f"è¿‡æ»¤å®Œæˆ: {matched_count}/{len(all_files)} ä¸ªæ–‡ä»¶åŒ¹é…")

        except Exception as e:
            print(f"é”™è¯¯ï¼šæ–‡ä»¶è¿‡æ»¤å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

        return matching_files

    def _matches_filter(self, filename, patterns):
        """Check if filename matches any of the filter patterns"""
        try:
            for pattern in patterns:
                if pattern and fnmatch.fnmatch(filename, pattern):
                    print(f"    âœ“ æ–‡ä»¶å '{filename}' åŒ¹é…æ¨¡å¼ '{pattern}'")
                    return True
            print(f"    âŒ æ–‡ä»¶å '{filename}' ä¸åŒ¹é…ä»»ä½•æ¨¡å¼ {patterns}")
            return False

        except Exception as e:
            print(f"è­¦å‘Šï¼šè¿‡æ»¤å™¨æ¨¡å¼åŒ¹é…å¤±è´¥: {str(e)}")
            return False

    def _is_image_file(self, filename):
        """Check if file is a supported image format"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.tif', '.webp', '.ico'}
        _, ext = os.path.splitext(filename.lower())
        is_image = ext in image_extensions
        print(f"    å›¾åƒæ ¼å¼æ£€æŸ¥: {filename} -> æ‰©å±•å: {ext} -> æ˜¯å›¾åƒ: {is_image}")
        return is_image

    def _try_decode_image(self, binary_data, relative_path):
        """Try to decode binary data as an image and convert to ComfyUI format"""
        try:
            print(f"    ğŸ–¼ï¸ å°è¯•è§£ç å›¾åƒ: {relative_path}")

            # Check if file extension suggests it's an image
            if not self._is_image_file(relative_path):
                print(f"    â­ï¸ è·³è¿‡éå›¾åƒæ–‡ä»¶: {relative_path}")
                return None

            print(f"    ğŸ“Š å›¾åƒæ•°æ®å¤§å°: {len(binary_data):,} å­—èŠ‚")

            # Try to open image with PIL
            image_stream = io.BytesIO(binary_data)
            pil_image = Image.open(image_stream)

            original_mode = pil_image.mode
            original_size = pil_image.size
            print(f"    âœ“ PILå›¾åƒåŠ è½½æˆåŠŸ: æ¨¡å¼={original_mode}, å°ºå¯¸={original_size}")

            # Convert to RGB if necessary (handle RGBA, grayscale, etc.)
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
                print(f"    ğŸ”„ é¢œè‰²æ¨¡å¼è½¬æ¢: {original_mode} -> RGB")

            # Convert PIL image to numpy array
            image_array = np.array(pil_image)
            print(f"    ğŸ“Š NumPyæ•°ç»„å½¢çŠ¶: {image_array.shape}, æ•°æ®ç±»å‹: {image_array.dtype}")

            # Convert to ComfyUI format: (H, W, C) with values in [0, 1]
            image_array = image_array.astype(np.float32) / 255.0
            print(f"    ğŸ”„ æ•°æ®ç±»å‹è½¬æ¢: uint8 -> float32, èŒƒå›´: [0, 255] -> [0, 1]")

            # Convert to PyTorch tensor
            image_tensor = torch.from_numpy(image_array)
            print(f"    âœ“ PyTorchå¼ é‡åˆ›å»ºæˆåŠŸ: å½¢çŠ¶={image_tensor.shape}, æ•°æ®ç±»å‹={image_tensor.dtype}")

            return image_tensor

        except Exception as e:
            # Not an image or failed to decode, silently ignore
            print(f"    âŒ å›¾åƒè§£ç å¤±è´¥: {relative_path} - {str(e)}")
            return None
